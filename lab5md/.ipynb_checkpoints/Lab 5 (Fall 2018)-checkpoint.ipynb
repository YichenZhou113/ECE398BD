{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab consists of two parts, one on background subtraction in videos, and the other on neural networks for MNIST digit classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary packages\n",
    "- opencv\n",
    "- tensorflow\n",
    "- numpy\n",
    "- matplotlib\n",
    "- IPython\n",
    "\n",
    "You will need to install `tensorflow` for this lab. Tensorflow 1.11.0 was used to develop this lab. You can use previous version, but might need to change some code. You can do this in a way similar to how you installed `opencv` in the last lab: for example, using `conda install tensorflow`.\n",
    "\n",
    "There are **6** exercises in this lab. Make sure to attempt all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3c38fc1b1ebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:blue\"> Part 1: Background Subtraction in Videos </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to [lecture notes](https://courses.engr.illinois.edu/ece398bd/fa2018/secure/lec8_Audio-Visual-Analytics.pdf) for the simple background modelling using a single Gaussian. You will develop relevant code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:blue\"> Exercise 0: Load the videos </span> \n",
    "\n",
    "Here's an example of how to load the video, display it, and then save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the video\n",
    "cap = cv2.VideoCapture(\"data/highway_trimmed.mp4\")\n",
    "\n",
    "# obtain the properties of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "property_id = int(cv2.CAP_PROP_FRAME_COUNT) \n",
    "video_length = int(cv2.VideoCapture.get(cap, property_id))\n",
    "print(property_id, video_length)\n",
    "\n",
    "# read the first frame\n",
    "_, first_frame = cap.read()\n",
    "first_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "height, width = np.array(first_gray).shape\n",
    "\n",
    "# display the first frame\n",
    "plt.figure()\n",
    "plt.imshow(np.array(first_gray), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# create a writer to save videos\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "writer = cv2.VideoWriter('video_original.mp4',\n",
    "                         fourcc,\n",
    "                         fps,\n",
    "                         (width, height), \n",
    "                         False)\n",
    "\n",
    "# go through the entire video and display + write the frames\n",
    "for itr in range(video_length-1):\n",
    "    _, frame_t = cap.read()\n",
    "    frame_t = cv2.cvtColor(frame_t, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    cv2.imshow('Frame', frame_t)\n",
    "    writer.write(frame_t)\n",
    "    key = cv2.waitKey(30)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "print('all frames covered')\n",
    "\n",
    "# release the reader, writer and opencv displays\n",
    "cap.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color:blue\"> Exercise 1: Estimate background and subtract background</span> \n",
    "\n",
    "As the first step, \n",
    "- Estimate the mean frame `mu` by going through the entire video.\n",
    "- Go through the entire video again, and subtract the estimated background to get the foreground. \n",
    "- Save this video as `video_exercise1_parking.mp4` or `video_exercise1_highway.mp4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:blue\"> Exercise 1(a): Estimate Background using Mean of Frames </span>  \n",
    "- Load the related video and extract its properties\n",
    "- Load the first frame for obtaining height and width\n",
    "- Estimate the mean frame `mu` by going through the entire video\n",
    "    - First, set `mu` to be the first frame\n",
    "    - Update using using `mu = (1-alpha)*mu + alpha*frame`\n",
    "    - You can set `alpha=0.1` for starting, and then play around with this parameter.\n",
    "- Display the estimated mean frame `mu` in an inline pyplot\n",
    "- We will use this mean image `mu` in the next step\n",
    "- You do not need to save any video in this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# read the video\n",
    "\n",
    "# obtain the properties of the video\n",
    "\n",
    "# read the first frame\n",
    "\n",
    "# set mu and alpha\n",
    "\n",
    "# display the first frame\n",
    "\n",
    "# go through the entire video and display+write the frames\n",
    "\n",
    "\n",
    "print('all frames covered')\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# imshow the mean mu as a figure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:blue\"> Exercise 1(b): Subtract background for all frames </span>  \n",
    "- Load the video again and extract its properties\n",
    "- Load the first frame for obtaining height and width\n",
    "- Use the estimated mean `mu` to remove the estimated background (Hint: use the function `cv2.absdiff()`. \n",
    "You may need to use the code \n",
    "\n",
    "`mu = cv2.convertScaleAbs(mu_t)`\n",
    "\n",
    "to ensure a right format in `cv2.absdiff()`)\n",
    "- You can further threshold the estimated background to remove noise using a threshold `thresh` as below:\n",
    "\n",
    "` _, difference = cv2.threshold(difference, thresh, 255, cv2.THRESH_BINARY)`\n",
    "   \n",
    "- Save the resulting video using the video writer to `video_exercise1_highway.mp4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# read the video\n",
    "\n",
    "# obtain the properties of the video\n",
    "\n",
    "# read the first frame\n",
    "\n",
    "# create a writer to save videos\n",
    "\n",
    "# set alpha and thresh\n",
    "\n",
    "# display the first frame\n",
    "\n",
    "# go through the entire video and display+write the frames\n",
    "\n",
    "print('all frames covered')\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color:blue\"> Exercise 2: Estimate frame mean and variance; subtract background on-the-fly </span> \n",
    "\n",
    "In this exercise, you will determine the mean and variance of the image frames on-the-fly. That is, you will determine the mean and variance as the frames change, and use this immediately for background subtraction. \n",
    "\n",
    "##### First 100 frames\n",
    "- To ensure that you have a good estimate of the mean and variance, you should use the first 100 frames for determining an initial estimate of the mean `mu` and variance `sigma`.\n",
    "\n",
    "##### Beyond 100 frames\n",
    "- Whenever a given pixel is more than `k=0.5` standard deviations away from the mean, i.e. when `frame > mu + k*sigma` or `frame < mu - k*sigma` you will consider this as the foreground. The remaining pixels will be the background.\n",
    "- Update the background `mu` using the `alpha` as done above\n",
    "- You will need to derive the update rule for `sigma` with respect to `alpha`. Include the update rule in the space below. Also specify clearly how you implement the update rule (i.e. what variables you use, and how). Note: the variance `sigma**2` is different from the standard deviation `sigma`. Tip: `sigma_N**2 ~ 1/N(sum_i [frame - mu]**2)` \n",
    "- Define `foreground_frame` which has non-zero values at pixels which are the foreground. That is, a pixel `[i,j]` will have the value such that `foreground_frame[i, j] = frame[i, j]` if the pixel `[i,j]` falls in the range `frame[i,j] > mu[i,j] + k*sigma[i,j]` or `frame[i,j] < mu - k*sigma[i,j]`\n",
    "- You can use `cv2.threshold(foreground_frame, 0, 255, cv2.THRESH_BINARY)` to generate the binary foreground images\n",
    "- You can process all pixels at once. You don't need `for` loops for this.\n",
    "- Finally, show the final `mu` and `sigma` as grayscale images.\n",
    "- Output the foreground video comprising of `foreground_frame`s for all frames to the file `video_exercise2_highway.mp4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Exercise 2(a): Update rule for sigma in terms of alpha: </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Exercise 2(b): Perform the background subtraction in real-time/on-the-fly </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "num_frames = 100\n",
    "\n",
    "# read the video\n",
    "\n",
    "# obtain the properties of the video\n",
    "\n",
    "# read the first frame\n",
    "\n",
    "# define mu and sigma. set mu to be the first frame, and sigma \n",
    "# as a zeros array of the same size as the first frame\n",
    "    \n",
    "# create a writer to save videos\n",
    "\n",
    "# set the alpha and k parameters\n",
    "\n",
    "# go through the entire video and display+write the frames\n",
    "for idx in range(num_frames):\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(mu_t, cmap='gray')\n",
    "plt.figure()\n",
    "plt.imshow(sigma_t, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "for idx in range(num_frames, video_length-1):\n",
    "\n",
    "    \n",
    "\n",
    "print('all frames covered')\n",
    "\n",
    "cap.release()\n",
    "writer2.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# imshow the final mu and sigma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color:blue\"> Exercise 3: Using mixture of gaussians for background subtraction </span> \n",
    "\n",
    "- In this exercise, repeat the background subtraction using the function  `cv2.createBackgroundSubtractorMOG2()` of opencv to subtract the background. \n",
    "- Play around with the parameters `history`, `threshold` and `detectShadows`\n",
    "- After reading a frame, the subtractor can be called by simply using `subtractor.apply(frame)` which outputs the foreground image\n",
    "- Output the foreground video comprising of the foreground frames to the file `video_exercise3_highway.mp4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the video\n",
    "\n",
    "# obtain the properties of the video\n",
    "\n",
    "# read the first frame\n",
    "\n",
    "# create a writer to save videos\n",
    "\n",
    "# define the MOG subtractor\n",
    "\n",
    "# go through the entire video and display+write the frames\n",
    "\n",
    " \n",
    "writer.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color:blue\"> Exercise 4: Another Video + Comparisons </span> \n",
    "\n",
    "####  <span style=\"color:blue\"> Exercise 4(a): Repeat the above three exercises for `parking_trimmed.mp4` </span> \n",
    "\n",
    "- Save the videos correspondingly to `video_exercise1_parking.mp4`, `video_exercise3_parking.mp4` and `video_exercise3_parking.mp4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:blue\"> Exercise 4(b): Comparing the subtractors </span> \n",
    "\n",
    "Answer the following questions using `Subtractor1`, `Subtractor 2` or `Subtractor 3` corresponding to the subtractors in the three previous exercises above:\n",
    "1. Which subtractor works best for the `highway_trimmed.mp4` video? **Why?**\n",
    "2. Which subtractor works best for the `parking_trimmed.mp4` video? **Why?**\n",
    "3. Which subtractor(s) evolve to consider the newly parked car in `parking_trimmed.mp4` as part of the background? **Why?**\n",
    "4. Note that the video `parking_trimmed.mp4` is a collection of snapshots and therefore not continuous. Which subtractor(s) perform well in this setting? That is, which subtractor(s) don't get affected by the low frame rate? **Why?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:blue\"> Part 2: Introduction to Neural Networks </span> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some tutorials\n",
    "\n",
    "There are plenty of online tutorials for tensorflow and deep learning. Feel free to explore and understand the working of tensorflow through external sources, including:\n",
    "- http://cs231n.github.io/convolutional-networks/\n",
    "- https://www.deeplearningbook.org/\n",
    "- https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/\n",
    "- https://brohrer.github.io/how_convolutional_neural_networks_work.html\n",
    "- https://www.linkedin.com/pulse/derivation-convolutional-neural-network-from-fully-connected-gad/\n",
    "- https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8\n",
    "\n",
    "This presents a playground for you to explore:\n",
    "- https://playground.tensorflow.org/\n",
    "\n",
    "Guides to tensorflow:\n",
    "- https://www.infoworld.com/article/3278008/tensorflow/what-is-tensorflow-the-machine-learning-library-explained.html \n",
    "- https://www.oreilly.com/learning/hello-tensorflow\n",
    "- https://www.tensorflow.org/tutorials/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color:blue\"> Exercise 5: This exercise introduces working with fully connected neural networks with `tensorflow` </span>\n",
    "\n",
    "- Ensure you have tensorflow correctly installed\n",
    "- Tensorflow runs networks by generating computation graphs, and updating the weights of the different connections in the graph during training\n",
    "- We will classify the MNIST dataset using a neural network of fully connected layers. Recall that a fully connected layer has a weight and bias term associated with every node of the previous layer to every node of the current layer. \n",
    "- Go through the code below. You will be asked to train the network for different settings and complete the `results` table and answer questions regarding the tensorflow implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST data\n",
    "# It's okay if this cell gives warnings.\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the computation graph of tensorflow\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# code to shuffle a batch during training\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "X_train, y_train, X_test, y_test = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "\n",
    "# flatten images to feed to a fully connected network\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) \n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28)\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "# split training into training and validation \n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of inputs of the network\n",
    "n_inputs = 28*28  # MNIST\n",
    "\n",
    "# number of nodes in the 1st hidden layer\n",
    "n_hidden1 = 300\n",
    "\n",
    "# number of nodes in the 2nd hidden layer\n",
    "n_hidden2 = 100\n",
    "\n",
    "# number of nodes at output: number of classes of MNIST\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# create a placeholder to feed input images and labels for training\n",
    "# these are fed to during training and testing time\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network \n",
    "\n",
    "# first layer takes as input X and maps to n_hidden1 number of nodes\n",
    "# the RELU activation is used\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                          activation=tf.nn.relu)\n",
    "\n",
    "# second layer takes as input the output of n_hidden1 nodes \n",
    "# and maps to n_hidden2 number of nodes\n",
    "# the RELU activation is used\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                          activation=tf.nn.relu)\n",
    "\n",
    "# final layer takes as input the output of n_hidden2 nodes \n",
    "# and maps to n_outputs number of nodes\n",
    "# output is scaled using softmax to get probabilities y_proba\n",
    "logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "y_proba = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function to use\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer and learning rate to use for the training\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for evaluation\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize all variables randomly \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the code block below takes about 1 minute for me. It can take you longer depending on the computational power of your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# number of epochs is the number of times all the images in \n",
    "# the dataset are accessed/used\n",
    "n_epochs = 20\n",
    "\n",
    "# batch size is the number of images used for \n",
    "# one step of the training\n",
    "batch_size = 50\n",
    "\n",
    "# the actual training\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if (epoch)%5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "# the final performance can be judged quantitatively by looking at the validation accuracy\n",
    "print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your UIN: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:blue\"> Exercise 5(a): \n",
    "    \n",
    "    - Re-run the training and report running time, number of parameters and validation accuracy for the following settings. \n",
    "    - Fill in the values in the table below. \n",
    "    - Use a learning rate as determined by your UIN. \n",
    "    - An example for number of paramaters is shown. Complete the calculation, and enter the final number. \n",
    "    - Feel free to change the batch_size and number_epochs to get a better trained network.\n",
    "    \n",
    "    `learning_rate` = 0.0001*(last_2_digits_of_UIN)\n",
    "   \n",
    "   For example, if my UIN ends in 98, I will run with the learning rate 0.0098"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hidden Layer 1 | Hidden Layer 2 | Learning Rate | Number of parameters |  Validation accuracy | Time to run (seconds) |\n",
    "|--------|-----|---|---|----|---|\n",
    "| 300 nodes  | 100 nodes |- |  `28*28*300 + 300*100 + 100*10` | - ||\n",
    "| 200 nodes   | 100 nodes | - | - |  - | -  ||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:blue\"> Exercise 5(b):\n",
    "    \n",
    "    - Re-run the training, and report running time, number of parameters and validation accuracy for the following settings.\n",
    "    - Fill in the values in the table below. \n",
    "    - Use the learning rate above multiplied by a factor of 10.\n",
    "    - An example for number of paramaters is shown. \n",
    "    - Complete the calculation, and enter the final number. \n",
    "    - Feel free to change the batch_size and number_epochs to get a better trained network.\n",
    "    \n",
    "    `learning_rate` = 0.001*(last_2_digits_of_UIN)\n",
    "   \n",
    "   For example, if my UIN ends in 98, I will run with the learning rate 0.098"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hidden Layer 1 | Hidden Layer 2 | Learning Rate| Number of parameters  | Validation accuracy | Time to run (seconds) |\n",
    "|--------|-----|---|----|---|---|\n",
    "| 300 nodes   | 100 nodes | - | `28*28*300 + 300*100 + 100*10` |  - | - ||\n",
    "| 200 nodes    | 100 nodes | - |- |  - | -  ||\n",
    "| 100 nodes    | 50 nodes | - | - | - | -  ||\n",
    "| 100 nodes    | 0 (layer removed) | - | - |  - | -  ||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:blue\"> Exercise 5(c): Answer the following questions about the tensorflow code above:\n",
    "    \n",
    "    1. How many connections/parameters (excluding bias) are there in the first setting of 300 nodes in hidden layer 1 + 100 nodes in hidden layer 2, between these two hidden layers?\n",
    "    2. What is an epoch?\n",
    "    3. What is the optimizer being used?\n",
    "    4. What is the loss being used?\n",
    "    5. What is the activation function being used?\n",
    "    6. What does a `dense` layer mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answers here:\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. \n",
    "5. \n",
    "6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color:blue\"> Exercise 6: This exercise introduces working with convolutional neural networks with `tensorflow` </span>\n",
    "    \n",
    "- We will classify the MNIST dataset using a neural network of fully connected layers. Recall that a convolutional layer has filters which operate on the image. \n",
    "- Go through the code below. You will be asked to train the network for different settings and complete the results table and answer questions regarding the tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "X_train, y_train, X_test, y_test = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "\n",
    "# flatten images to feed to a fully connected network\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28, 28, 1) \n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28, 28, 1)\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "# split training into training and validation \n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of inputs of the network\n",
    "n_inputs = 28, 28  # MNIST\n",
    "\n",
    "# number of filters and filter size to be used for the first convolution layer\n",
    "n_filters1 = 32\n",
    "k_size1 = 16\n",
    "\n",
    "# number of filters and filter size to be used for the second convolution layer\n",
    "n_filters2 = 16\n",
    "k_size2 = 8\n",
    "\n",
    "# number of nodes at output: number of classes of MNIST\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# create a placehold to feed input images and labels for training\n",
    "# these are fed to during training and testing time\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 28, 28, 1), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "# define the network \n",
    "\n",
    "# first layer takes as input X and maps to n_hidden1 number of nodes\n",
    "hidden1 = tf.layers.conv2d(X, n_filters1, k_size1, activation=tf.nn.relu)\n",
    "hidden2 = tf.layers.conv2d(hidden1, n_filters2, k_size2, activation=tf.nn.relu)\n",
    "\n",
    "# flatten the final maps\n",
    "fc1 = tf.contrib.layers.flatten(hidden1)\n",
    "\n",
    "# map to n_outputs number of nodes\n",
    "# output is scaled using softmax to get probabilities y_proba\n",
    "logits = tf.layers.dense(fc1, n_outputs, name=\"outputs\")\n",
    "y_proba = tf.nn.softmax(logits)\n",
    "\n",
    "# define the loss function to use\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer and learning rate to use for the training\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for evaluation\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the code block below takes about 3 minutes for me. It can take you longer depending on the computational power of your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# initialize all variables randomly \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# number of epochs is the number of times all the images in \n",
    "# the dataset are accessed/used\n",
    "n_epochs = 5\n",
    "\n",
    "# batch size is the number of images used for \n",
    "# one step of the training\n",
    "batch_size = 10\n",
    "\n",
    "# the actual training\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "# the final performance can be judged quantitatively by looking at the validation accuracy\n",
    "print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your UIN: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:blue\"> Exercise 6(a): \n",
    "    \n",
    "    \n",
    "    - Re-run the training, number of parameters, and report running time and validation accuracy for the following settings.\n",
    "    - Fill in the values in the table below. \n",
    "    - An example for number of paramaters is shown. Complete the calculation, and enter the final number. \n",
    "    - You can use the same kernel/filter size for both convolutional layers. \n",
    "    - Feel free to change the batch_size and number_epochs to get a better trained network.\n",
    "    - Use the learning rate above multiplied by a factor of 10. For example, if my UIN ends in 98, I will run with the learning rate 0.098\n",
    "        \n",
    "    `learning_rate` = 0.001*(last_2_digits_of_UIN)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hidden Layer 1 | Hidden Layer 2 | Learning Rate | Number of Filter Parameters | Validation accuracy | Time to run (seconds) |\n",
    "|--------|-----|---|---|----|---|\n",
    "| 32 filters   | 16 filters  | - | = `32*16 + 16*8`   - | - ||\n",
    "| 32 filters   | 8 filters | - |  - | -  ||\n",
    "| 16 filters   | 8 filters | - |  - | -  ||\n",
    "| 8 filters    | 4 filters | - |  - | -  ||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:blue\"> Exercise 6(b): Answer the following questions comparing the fully connected network with the convolutional network:\n",
    "    \n",
    "    1. What is the main advantage of using a convolutional network?\n",
    "    2. Which network do you think performs better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested, you can write code to use the trained networks and run test images through them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes all labs for this section of the course. Hope you learnt useful ideas and concepts. Don't forget to fill out the feedback survey which will be posted soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
