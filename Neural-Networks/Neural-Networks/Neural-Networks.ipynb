{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks for regression via Tensorflow\n",
    "\n",
    "### Agenda:\n",
    "1. What is nonlinear regression?\n",
    "\n",
    "\n",
    "2. Learn how to do linear and nonlinear regression using Tensorflow. \n",
    "\n",
    "    a. How do you set up the computation graph? \n",
    "    \n",
    "    b. How do you train and test the model?\n",
    "   \n",
    "   \n",
    "2. Explain the role of activation functions.\n",
    "\n",
    "    a. No activation function means you are doing linear regression.\n",
    "    \n",
    "    b. What activation function would you choose?\n",
    "\n",
    "### What is nonlinear regression?\n",
    "\n",
    "Given $X$ and $Y$ data, find a function $F$ that best explains the relationship between $X$ and $Y$, i.e., find $F$ such that $Y \\approx F(X)$. Regression is the act of finding a suitable $F$. When $F$ is restricted to be linear, you are performing linear regression. It is the multivariate form of your familiar question: \"fit the best line to given data\". When $F$ can be nonlinear, finding that function $F$ is called nonlinear regression.\n",
    "\n",
    "Given: $({X}_1, {Y}_1), \\ldots, ({X}_n, {Y}_n)$. \n",
    "\n",
    "Objective: Find $F$ that minimizes the error between $Y$ and $F(X)$ on the data, i.e., solve\n",
    "$$ \\underset{F}{\\text{minimize}}\\ \\ \\frac{1}{n}\\sum_{i=1}^n \\| {Y}_i - F({X}_i) \\|^2. $$\n",
    "\n",
    " \n",
    "### What is a neural network and how can it do nonlinear regression?\n",
    "A neural network defines a sequence of operations on the input that produces an output. In the above diagram, your data $X$ goes through the neural network to produce the output $Y$. What does each layer represent? With the output of the last layer, it performs an affine transformation and then passes it through an **activation function** to compute the output of the current layer.\n",
    "\n",
    "<img src=\"neuralNet.png\"  width=\"700\">\n",
    "\n",
    "In essence, a neural network **parameterizes** the function $F$ in terms of the weights and biases in the neural network. In the above figure, the parameters are given by\n",
    "$$\\theta := ( W^1, b^1, W^2, b^2 ).$$\n",
    "Call this parametric representation $F_\\theta$. Then, the nonlinear regression via a neural network seeks $\\theta$ that solves\n",
    "$$ \\underset{\\theta}{\\text{minimize}}\\ \\ J(\\theta) := \\frac{1}{n}\\sum_{i=1}^n \\| {Y}_i - F_\\theta({X}_i) \\|^2. $$\n",
    "\n",
    "\n",
    "### How does a neural network optimize over $\\theta$?\n",
    "\n",
    "There are a variety of optimizer routines that one can use, the simplest among them is **stochastic gradient descent**. You sample one among your $n$ data points and sequentially update the parameters in $\\theta$ as follows. \n",
    "$$ \\theta_{k+1} := \\theta_{k} - \\alpha_k \\nabla_\\theta \\| {Y}_i - F_\\theta({X}_i) \\|^2, $$\n",
    "starting from a possibly random initial parameter vector $\\theta_0$. Here, $\\alpha$'s define a sequence of stepsizes, and $\\nabla J$ stands for the gradient of $J$.\n",
    "\n",
    "1. Can you use one data point at a time to perform an update? If so, how should you sample the data?\n",
    "2. Can you update $\\theta$ using a batch of gradients computed on a batch of data? \n",
    "3. Can you have different step-sizes for different parameters within $\\theta$?\n",
    "4. How fast does it converge?\n",
    "\n",
    "Machine learning research has focussed on each of these questions both using theoretical analysis and extensive simulation studies.\n",
    "\n",
    "\n",
    "### In the parametric description of $F$, what is an activation function?\n",
    "\n",
    "Without activation function, the output of the neural network is a linear function of $X$. To understand that statement, assume in the above network that activation function is identity, and therefore, we have $Z = X W^1 + b^1$, and \n",
    "$$ Y = Z W^2 + b^2 = ( X W^1 + b^1 ) W^2 + b^2 = X (W^1 W^2) + b^1 W^2 + b^2, $$\n",
    "a linear function of $X$. Utilizing the same logic, you can show that no matter how many layers you have or how many neurons you have in these layers, the output of a neural network is linear in $X$ without an activation function. You need a nonlinear activation function to learn complicated nonlinear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Nonlinear regression via Tensorflow.\n",
    "\n",
    "Construct $n=1000$ data points $({X}_1, {Y}_1), \\ldots ({X}_n, {Y}_n)$, where ${Y}_i = F({X}_i)$. Obtain the ${X}$'s by sampling uniformly between -5 and 5, n times.\n",
    "\n",
    "Start with the customary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data in Xbar and Ybar.\n",
    "\n",
    "n = 1000\n",
    "trainX = np.random.uniform(-5, 5, size=[n, 1])\n",
    "F = lambda x: np.sin(x) + 2 * np.cos(2 * x)\n",
    "trainY = np.array([F(x) for x in trainX])\n",
    "\n",
    "# Draw a scatter plot of the sampled points.\n",
    "plt.figure(1)\n",
    "plt.scatter(trainX, trainY)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the test data.\n",
    "\n",
    "Create a data set of $m=200$ points from $[-5, 5]$ on which you will test the accuracy of the regressor. Always keep the training and testing data different. If you don't and you seek to minimize training error, you may overfit your regressor to your training samples. Overfitting means that you fit the regressor so well on the training set, that it starts to perform way worse on samples it has never seen before. That is, your learnt function does not accurately capture the true relationship between $X$ and $Y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 200\n",
    "testX = np.random.uniform(-5, 5, size=[m, 1])\n",
    "testY = np.array([F(x) for x in testX])\n",
    "\n",
    "plt.figure(1)\n",
    "plt.scatter(testX, testY)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tensorflow to learn the function. \n",
    "\n",
    "Tensorflow allows you to construct a neural network as a computation graph. When you define the computation graph, it does not compute anything, but rather defines the sequence of operations that you will perform on the data. The data you will supply should be defined as placeholders, and the parameters defining the neural network should be defined as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the structure of the neural network as a computation graph in Tensorflow.\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "nHidden = 15\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal(shape=[1, nHidden]))\n",
    "b1 = tf.Variable(tf.truncated_normal(shape=[nHidden]))\n",
    "Z1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal(shape=[nHidden, 1]))\n",
    "b2 = tf.Variable(tf.truncated_normal(shape=[1]))\n",
    "Yhat = tf.matmul(Z1, W2) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few comments about the above code:\n",
    "\n",
    "1. $X$ will only be defined at runtime, and is left unspecified, except possibly its size. Notice that the first among the shape parameters is 'None'. This indicates to Tensorflow that we may pass multiple $X$'s to train or test the neural network. The second shape parameter being 1, our input data is scalar.\n",
    "\n",
    "2. How many hidden layers does this neural network have? There is one hidden layer with 15 neurons.\n",
    "\n",
    "3. What constitutes the parameters $\\theta$ for this neural network that we aim to optimize over? Weights W1, W2 and biases b1, b2.\n",
    "\n",
    "4. The activation function used here is 'relu'. We will learn more about activation functions.\n",
    "\n",
    "### Define the optimization routine for training the neural network.\n",
    "\n",
    "We aim to find $\\theta$ that minimizes the discrepancy between 'trainY' and $F_\\theta$ applied on 'trainX'. Define the target $Y$ as a placeholder and define an optimizer to minimize the mean squared distance between the neural network predictions $\\hat{Y}$ and its target $Y$. Similar to $X$, the placeholder $Y$ can have variable shape, depending upon how many data points are used to train or test the neural network. Therefore, define the first argument in its shape parameter as 'None', and the second arguemnt as one. \n",
    "\n",
    "**Question.** Why is the second argument in the shape of $Y$ equal to 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "# Define a loss function.\n",
    "loss = tf.losses.mean_squared_error(labels=Y, predictions=Yhat)\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.35).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Can you relate 'learning_rate' to our iterative description of how $\\theta$'s are updated?\n",
    "\n",
    "### Define a metric to judge the accuracy of prediction via neural network.\n",
    "\n",
    "To evaluate the performance of your regressor, you need to define an accuracy metric. A commonly used metric for nonlinear regression is \"root mean squared error\" (RMSE). On any test data, if the neural network produces $\\hat{Y}_1, \\ldots, \\hat{Y}_m$, and the true values are $Y_1, \\ldots, Y_m$, then the RMSE of the prediction is given by\n",
    "$$ \\text{RMSE} = \\sqrt{\\sum_{i=1}^m \\frac{(Y_i - \\hat{Y}_i)^2}{N}}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, accuracy = tf.metrics.root_mean_squared_error(labels=Y, predictions=Yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Why don't we use the function 'accuracy' as the 'loss' function and seek to minimize the RMSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier in a Tensorflow session.\n",
    "\n",
    "Having defined the computation graph, we now train the neural network. In a session, you initialize an instance of the computation graph, and pass the data ${X}, {Y}$ to it multiple times (called 'epochs'), and run the optimizer function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "\n",
    "    # Initialize the computation graph. \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    print(\"Started the training module.\")\n",
    "\n",
    "    # Define the number of epochs.\n",
    "    nEpochs = 4000\n",
    "    \n",
    "    for epoch in range(nEpochs):\n",
    "\n",
    "        lossEpoch = 0\n",
    "        # In each epoch, use 'optimizer' to reduce the 'loss' over the entire data. Make sure to pass\n",
    "        # the appropriate data to the placeholders.\n",
    "        _, lossEpoch = sess.run([optimizer, loss], feed_dict={X: trainX, Y: trainY})\n",
    "         \n",
    "        print(\"Epoch: %d, Loss: = %1.1f\" % (epoch + 1, lossEpoch))\n",
    "\n",
    "    print(\"End of training process...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't think your neural network has converged, play with the learning rate, number of epochs, activation functions, etc.\n",
    "\n",
    "#### Test the accuracy on test data and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # You are still inside the session. Notice the indentation!\n",
    "    \n",
    "    # Output the accuracy of the regressor on the test data.\n",
    "    predictedY, accuracyOfPrediction = sess.run([Yhat, accuracy], feed_dict={X: testX, Y: testY})\n",
    "    print(\"RMSE of regressor on test data = %1.2f\" % accuracyOfPrediction)\n",
    "    \n",
    "    plt.figure(2)\n",
    "    plt.scatter(testX, testY)\n",
    "    plt.scatter(testX, np.array(predictedY))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises: Check the result after making each of these changes. Make a note of your observations. Change back to the original setting for the next exercise.\n",
    "\n",
    "1. Change the number of neurons in the hidden layer to 3.\n",
    "2. Change back the number of hidden neurons to 15 and then remove the activation function in the hidden layer.\n",
    "3. Keep the activation in the hidden layer as 'relu', and additionally use a relu activation on the output layer.\n",
    "4. Remove activation in the output layer, but use a 'tanh' activation in the hidden layer.\n",
    "5. Vary the learning rate and check the quality of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
